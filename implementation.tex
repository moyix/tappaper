\section{Implementation}
\label{sec:implementation}

In this section, we describe both the dynamic analysis platform employed to
build TZB, but also TZB-specific algorithmic and data-structure solutions.

\subsection{PANDA}
\label{sec:implementation:subsec:panda}

TZB makes extensive use of the Platform for Architecture-Neutral Dynamic
Analysis (PANDA), which was developed by the authors in collaboration
with (anonymized).

PANDA is based upon version 1.0.1 of the QEMU machine
emulator~\cite{Bellard:2005}. QEMU is an excellent and common choice for
whole-system dynamic analysis for two main reasons. First, performance
is good (about 5x slowdown over native). Second, every basic block of
guest code is disassembled by the host in order to emulate, which means
that there are opportunities to interpose analyses at the basic block or
even instruction level, if desired. QEMU lowers instructions to an
intermediate language (IL) in order to employ a single back-end code
generator, the Tiny Code Generator (TCG). This IL means dynamic analyses
can potentially be written once and re-used for all 14 architectures
supported by QEMU. Further, this version of QEMU is modern enough to be
able to boot and run modern operating systems such as Windows 7 (earlier
versions of QEMU such as 0.9.1 cannot).

There are three main aspects to PANDA that make it very convenient for
building dynamic analyses. First, PANDA provides a plug-in architecture that
readily permits writing guest analyses in C and C++. Plug-in code is
executed from a number of standard callback locations: before and after
basic blocks, memory read and writes, etc. This is not unlike the
schemes employed in other whole-system dynamic analysis platforms such
as BitBlaze~\cite{Song:2008bitblaze} and S2E~\cite{Chipounov:2011s2e}.
In addition, plugins can export functionality that can then be used in
other plugins, allowing complex behavior to be built up from simple
components. From a software engineering perspective, PANDA's plugin
architecture allows the various analyses supported by TZB to be cleanly
separated from the main emulator, which makes for a much more
comprehensible and maintainable codebase.

The second aspect of PANDA that makes it an excellent dynamic analysis
platform is nondeterministic record and replay (RR). In our formulation
of RR, we begin a recording by invoking QEMU's built-in snapshot
capability. Subsequently, we record all inputs to the CPU, including
INs, interrupts, and DMA. Recording imposes a small overhead (10-20\%)
but not enough to perturb execution. During replay, we revert to a
snapshot and proceed to pull CPU inputs from a log when required.
Unlike many other RR schemes, we do not record and replay device inputs,
which means we cannot ``go live'' at any point during replay. But we
can perform repeated replays of an entire operating system under
arbitrary instrumentation load without worrying about this perturbing
application or operating system operation. This capability is vital to
TZB: without record and replay, the heavyweight analyses we perform
would make the system unusably slow.

The final aspect of PANDA worth mentioning is its integration of LLVM.
QEMU lowers basic blocks of guest code to its own IL, which PANDA can,
additionally, re-render as basic blocks of LLVM code via a module 
extracted from S2E. We omit further discussion of this capability
as it is not used by TZB.

\subsection{Callstack Monitoring}
\label{sec:implementation:subsec:callstack}

As explained in Section~\ref{sec:tapdef}, tap points need information
about the calling context. Keeping track of this information requires
some knowledge about the CPU architecture on which the OS is running,
and so we decided to encapsulate this task into a single plugin. TZB's
other analyses can then query the current call stack to arbitrary depth
by invoking \texttt{get\_callers} and not worry about the details
described in this section.

To track call stack information, the \texttt{callstack} plugin examines
each basic block as it is translated, looking for an
(architecture-specific) call instruction (currently, we look for
\texttt{call} on x86 and \texttt{bl} and \texttt{mov lr, pc} on ARM). If
the block includes a call instruction, then we push the return address
onto a shadow stack after each time that block executes.

Detecting the return from a function does not require any
architecture-specific code. Before the execution of every basic block,
we check whether the address we are about to execute is at the top of
the stack; if so, we pop it. We only need to check the starting address
of the basic block, because by definition a return terminates a basic
block, so the return address will always fall at the beginning of a
block.

We note that these techniques may fail if traditional call-return
semantics are violated. For example, if a program emulated calls and
returns by manually pushing the return address and using a direct jump,
it would not be detected as a call. However, for non-malicious
compiler-generated code, we have found that the algorithm described here
works well.

\subsection{Fixed String Searching}
\label{sec:implementation:subsec:stringsearch}

Searching for fixed strings is one of the most effective tools for
finding useful tap points. Because we have to sift through many
gigabytes of data that pass through tap points during any given
execution, it is vital that string search be efficient in both time and
space.

To satisfy these constraints, we developed a string matching algorithm
(implemented as a PANDA plugin called \texttt{stringsearch}) that
requires only one byte of memory per search string and per tap point.
This one-byte counter tracks, for a given tap point, how many bytes of
the search string have been matched by the data seen at the tap point so
far. Whenever a byte is read from or written to memory, we can check
what the next byte in the search string is using this position, and
compare it to the byte passing through the tap point. If it matches, the
counter is incremented; if it does not match, the counter is reset to
zero. When the counter equals the length of the search string, we know
that the search string has passed through the tap point, and we report a
match. Note that because the counter is only one byte, our matcher only
supports strings up to 256 bytes long; this cap could be easily raised
to 65,536 bytes by using a two-byte counter, at the cost of doubling the
memory requirements. Thus far, 256-byte strings have been more than
sufficient.

This effectively implements a very simple deterministic finite automaton
(DFA) matcher. Indeed, we believe that it should be possible to
efficiently implement a streaming basic regular expression matcher that
only an amount of memory logarithmic in the number of states needed to
represent the expression. We leave this generalization to future work,
however.

\subsection{Statistical Search and Clustering}
\label{sec:implementation:subsec:bigram}

Collecting bigram statistics on data that passes through each tap point
is an efficient way to enable ``fuzzy'' search based based on some
training examples, as well as enabling clustering. To implement this
we collect bigram statistics for all tap points seen in execution, as
well as for the exemplar; the data seen at each tap point is thus
represented as a sparse vector with 65,536 elements (one for each
possible pair of bytes).

To search, we can then sort the tap points seen by taking the distance
(according to some metric) from the exemplar. For our metric, we have
chosen to use Jensen-Shannon divergence~\cite{Lin:2006fk}, which is a
smoothed and symmetrized version of the classic Kullback-Leibler
divergence~\cite{Kullback:1951uq} (also known as information gain). We
also examined the Euclidean and cosine distance metrics, but found their
performance to be consistently worse than Jensen-Shannon. Jensen-Shannon
divergence between two probability distributions $P$ and $Q$ is defined
as:

\[
JSD(P, Q) = H \left ( \frac{P+Q}{2} \right ) - \frac{H(P)+H(Q)}{2}
\]

\noindent where $H$ is Shannon entropy.

Bigram collection is done by maintaining, for each tap point, two pieces
of information: (1) the last byte that passed through by the tap point,
so that we can see bigrams that span a single memory access; (2) a
histogram of all byte pairs seen at the tap point. The latter of these
must be maintained sparsely: because our bigrams are based on bytes, a
dense histogram would require 65,536 integers' worth of storage per tap
point. Given that most of the executions examined in this paper contain
upwards of 500,000 tap points, this would require more than 120GB of
memory, which is clearly infeasible (and wasteful, since most of those
entries would be zero).

Instead, we store the histogram sparsely, using a C++ Standard Template
Library \texttt{std::map<uint16\_t,int>}. This keeps memory usage down
without sacrificing any accuracy, but it does introduce some extra
complexity when processing the resulting histograms, as our search
software must support sparse vectors rather than simple arrays. Because
of this additional complexity, we opted to implement the search and
clustering algorithms ourselves, after some initial prototyping using
SciPy's \texttt{sklearn} toolkit.

Our clustering is based on the venerable $k$-means
algorithm~\cite{Steinhaus:1956kx}, but using the Jensen-Shannon
divergence described in the previous section. As in the statistical
search case, we use bigram statistics for our feature vectors.
Initialization uses the KMeans++ algorithm~\cite{Arthur:2007ve}, which
helps guarantee that the initial cluster centers are widely separated.
We evaluate the performance of this clustering compared to an expert
labeling in Section~\ref{sec:eval:subsec:cluster}.

Our statistical search tool is implemented in 246 lines of C++, and
computes the Jensen-Shannon divergence between a training histogram
(dense) and a set of sparse histograms. Our K-Means clustering tool is
481 lines of C++ code, and outputs a clustering of the sparse histograms
using Jensen-Shannon divergence as a distance metric.\footnote{The use
of this distance metric is justified theoretically because
Jensen-Shannon distance is a Bregman divergence~\cite{Banerjee:2005qf}
and empirically because our clustering typically converges after around
30 iterations.} Both tools are multithreaded, which greatly speeds up the
computation.

\subsection{Finding SSL/TLS Keys}
\label{sec:implementation:subsec:keyfind}

We have also implemented a PANDA plugin called \texttt{keyfind}, which
locates tap points that write SSL/TLS master secrets. The SSL/TLS master
secret is a 48-byte string from which an SSL/TLS-encrypted session's
keys are derived; thus, if a tap point that writes the master key can be
found, encrypted network traffic can be decrypted and analyzed.

The plugin operates on a recording in which a program initiates an
encrypted connection to some server and an encrypted packet sent by the
client (captured using, e.g., \texttt{tcpdump}). The \texttt{keyfind}
uses each 48 bytes accessed at each tap point as a trial decryption key
for a sample packet sent by the client. If the decrypted packet's
Message Authentication Code (MAC) verifies that the packet was decrypted
correctly then we can conclude that the tap point can be used to decrypt
SSL/TLS connections made by the program under inspection. In
Section~\ref{sec:eval:subsec:sslmal} we show how this technique can be
used to spy on connections made by the Sykipot malware, without
performing a (potentially detectable) man in the middle attack.
